# ìœ ë‹ˆì˜¨ â†’ ì •ì œ â†’ ì¤‘ë³µì œê±° â†’ í† í°í™”(ë¶ˆìš©ì–´ ì œê±°)
import os, re, glob, unicodedata, hashlib
import pandas as pd
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(r"D:/reviews")  # íŒŒì¼ ê²½ë¡œ
TS = datetime.now().strftime("%Y%m%d_%H%M%S") # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ íŒŒì¼ëª… ë’¤ì— ë‚ ì§œ/ì‹œê°„

# ë¦¬ë·° íŒŒì¼ í•©ì¹˜ê¸° (ìœ ë‹ˆì˜¨)
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
paths = sorted(glob.glob(str(BASE_DIR / "*_new_reviews*.csv")))
if not paths:
    raise FileNotFoundError("ë¦¬ë·° CSVê°€ ì—†ìŠµë‹ˆë‹¤ (*_new_reviews*.csv).")

dfs = []
for p in paths:
    df = pd.read_csv(p)
    # ì»¬ëŸ¼ ì„¤ì •
    # íŒŒì¼ë§ˆë‹¤ ì»¬ëŸ¼ ëª…ì´ ë‹¤ë¥¸ê²Œ ì§€ì •ë˜ì–´ìˆì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ë‘ ê°œ ì¤‘ íƒ1
    cols = [c.lower() for c in df.columns]
    df.columns = cols
    if "place_name" not in df.columns:
        df["place_name"] = Path(p).stem.split("_new_reviews_")[0] # íŒŒì¼ëª…ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    if "review_text" not in df.columns:
        cand = [c for c in df.columns if c in ("ë³¸ë¬¸","review_text")] # ë³¸ë¬¸ or review_test ê°€ì ¸ì˜¤ê¸°
        if cand: df["text"] = df[cand[0]]
        else: raise ValueError(f"text ì»¬ëŸ¼ ì—†ìŒ: {p}")
    if "visit_date" not in df.columns:
        cand = [c for c in df.columns if c in ("ë°©ë¬¸ì¼","visit_date")] # ë°©ë¬¸ì¼ or visit_date ê°€ì ¸ì˜¤ê¸°
    if "visit_count" not in df.columns:
        cand = [c for c in df.columns if c in ("ë°©ë¬¸íšŸìˆ˜","visit_count")]  # ë°©ë¬¸íšŸìˆ˜ or visit_count ê°€ì ¸ì˜¤ê¸°
      
    dfs.append(df[["place_name","visit_date","visit_count","review_text"]])

# ìœ ë‹ˆì˜¨
raw = pd.concat(dfs, ignore_index=True)

# í…ìŠ¤íŠ¸ ì •ê·œí™” + ì¤‘ë³µ ì œê±°
def normalize_text(s: str) -> str:
    s = str(s or "").strip()
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    return s

# ë‚ ì§œ ì •ê·œí™”
def norm_date(s):
    ts = pd.to_datetime(s, errors="coerce")
    return ts.dt.date if isinstance(ts, pd.Series) else (ts.date() if pd.notna(ts) else pd.NaT)

# ë°©ë¬¸íšŸìˆ˜ ì •ê·œí™”
def norm_count(x):
    if pd.isna(x): 
        return pd.NA
    m = re.search(r"\d+", str(x))
    return int(m.group()) if m else pd.NA

# í…ìŠ¤íŠ¸ ì •ê·œí™”
raw["review_text"] = raw["review_text"].map(normalize_text)

# ë‚ ì§œ ì •ê·œí™”: datetimeìœ¼ë¡œ íŒŒì‹± í›„ 'ì¼(date)' ë‹¨ìœ„ë§Œ ì‚¬ìš©
raw["visit_date"] = raw["visit_date"].map(norm_date)

# ë°©ë¬¸íšŸìˆ˜ ì •ê·œí™”: ìˆ«ìë§Œ ì¶”ì¶œ, pandas nullable ì •ìˆ˜í˜•(Int64)ë¡œ ë³´ê´€
raw["visit_count"] = raw["visit_count"].map(norm_count).astype("Int64")

# ===== 4) ì¤‘ë³µ ì œê±° (ê°€ê²Œ+í…ìŠ¤íŠ¸+ë°©ë¬¸ì¼+ë°©ë¬¸íšŸìˆ˜ ì¡°í•© ê¸°ì¤€) =====
raw = (raw
       .drop_duplicates(subset=["place_name", "review_text", "visit_date", "visit_count"])
       .reset_index(drop=True))

# 3) í† í°í™”(kiwipiepy ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì •ê·œì‹ í´ë°±) + ë¶ˆìš©ì–´ ì œê±°
try:
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    def tokenize(text):
        toks=[]
        for t in kiwi.tokenize(text, normalize_coda=True):
            lemma = t.form if t.lemma is None else t.lemma
            if re.fullmatch(r"[ê°€-í£A-Za-z0-9]+", lemma):
                toks.append(lemma)
        return toks
    TOKENIZER = "kiwipiepy"
except Exception:
    def tokenize(text):
        text = re.sub(r"http\S+|www\.\S+", " ", text)
        text = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text)
        toks = [w for w in re.split(r"\s+", text) if w]
        return toks
    TOKENIZER = "regex"

# ê°„ë‹¨ ë¶ˆìš©ì–´(ì›í•˜ë©´ ê³„ì† ì¶”ê°€)
# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë³´ê°•
STOP = set("""
ì€ ëŠ” ì´ ê°€ ì„ ë¥¼ ì— ì—ì„œ ìœ¼ë¡œ ë¡œ ì™€ ê³¼ ë„ ë§Œ ê¹Œì§€ ë¶€í„° ì˜ ì—ê²Œ ê»˜ì„œ í•œí…Œ 
í•˜ê³  í•˜ë‹¤ ìˆë‹¤ ì—†ë‹¤ ë˜ë‹¤ ì´ë‹¤ ì•„ë‹ˆë‹¤ ê°™ë‹¤ ë‹¤ë¥´ë‹¤ í¬ë‹¤ ì‘ë‹¤ ì¢‹ë‹¤ ë‚˜ì˜ë‹¤
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ê·¸ëŸ°ë° í•˜ì§€ë§Œ ë˜í•œ ë˜ëŠ” ê·¸ëƒ¥ ì¢€ ì•„ì£¼ ì§„ì§œ ì •ë§ ë§¤ìš° ë„ˆë¬´
ìš”ì¦˜ ì˜¤ëŠ˜ ì–´ì œ ë‚´ì¼ ì´ë²ˆ ì§€ë‚œ ë‹¤ìŒ ë˜ ë‹¤ì‹œ ê³„ì† í•­ìƒ ê°€ë” ë•Œë•Œë¡œ ìì£¼
ê²ƒ ìˆ˜ ë•Œ ê³³ ì  ê°œ ëª… ì› ì‹œê°„ ë¶„ ì¼ ì›” ë…„ ë²ˆì§¸ ì •ë„ ì•½ 
""".split())

# í† í° ì •ì œ í•¨ìˆ˜ ê°œì„ 
def clean_tokens(toks):
    out = []
    for w in toks:
        # ì¡°ì‚¬ ì œê±° (ë” í¬ê´„ì ìœ¼ë¡œ)
        w2 = re.sub(r"(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì˜|ê»˜ì„œ|í•œí…Œ|ì—ê²Œ)$", "", w)
        
        # ì–´ë¯¸ ì œê±° (ì¼ë¶€)
        w2 = re.sub(r"(ìŠµë‹ˆë‹¤|ì„¸ìš”|ì–´ìš”|ì•„ìš”|í•´ìš”|ì§€ìš”|ë„¤ìš”|ì˜ˆìš”|ì´ì—ìš”)$", "", w2)
        
        if not w2 or w2 in STOP or len(w2) < 2:  # 2ê¸€ì ë¯¸ë§Œ ì œê±°
            continue
            
        # ìˆ«ìë§Œ ìˆëŠ” í† í° ì œê±°
        if re.fullmatch(r"\d+", w2):
            continue
            
        out.append(w2)
    return out

raw["tokens"] = raw["review_text"].apply(lambda s: clean_tokens(tokenize(s)))

# ì¸ë±ìŠ¤ ë¦¬ì…‹ í›„ ë²ˆí˜¸ ë§¤ê¸°ê¸°
raw = raw.reset_index(drop=True)
raw["review_number"] = raw.index + 1

# BASE_DIR = "D:/SY ì—…ë¬´/ê¸°íƒ€/ê°œì¸ê³¼ì œ/ë¦¬ë·°/raw_data"
# 4) ì €ì¥(ë§ˆìŠ¤í„° ì›ë¬¸ + í† í°)
out_master = BASE_DIR / f"reviews_master_{TS}.csv"
raw[["place_name","visit_date","visit_count","review_text","tokens"]].to_csv(out_master, index=False, encoding="utf-8-sig")

out_tokens = BASE_DIR / f"reviews_master_tokens_{TS}.csv"
# í† í°ì€ ê³µë°±-ì¡°ì¸ ë¬¸ìì—´ë„ ê°™ì´ ì €ì¥í•´ë‘ë©´ TF-IDF ë°”ë¡œ ê°€ëŠ¥
tmp = raw.copy()
tmp["tokens_join"] = tmp["tokens"].apply(lambda x: " ".join(x))
tmp[["place_name","review_number","visit_date","visit_count","tokens_join"]].to_csv(out_tokens, index=False, encoding="utf-8-sig")



print("ğŸ’¾ ì €ì¥:", out_master.name, "|", out_tokens.name)
print("í† í°í™” ë°©ì‹:", TOKENIZER)
# ë°ì´í„° í˜„í™© íŒŒì•…
print("=== ë°ì´í„° í˜„í™© ===")
print(f"ì´ ë¦¬ë·° ìˆ˜: {len(raw):,}")
print(f"ê°€ê²Œ ìˆ˜: {raw['place_name'].nunique()}")
print(f"ê°€ê²Œë³„ ë¦¬ë·° ìˆ˜:\n{raw['place_name'].value_counts().head(10)}")

# í† í° í˜„í™©
all_tokens = [token for tokens in raw['tokens'] for token in tokens]
print(f"\nì´ í† í° ìˆ˜: {len(all_tokens):,}")
print(f"ìœ ë‹ˆí¬ í† í° ìˆ˜: {len(set(all_tokens)):,}")

# ë¹ˆë„ ë†’ì€ í† í° í™•ì¸ (ë¶ˆìš©ì–´ ì¶”ê°€ ì œê±° í•„ìš”í•œì§€ ì²´í¬)
from collections import Counter
token_freq = Counter(all_tokens)
print(f"\nìƒìœ„ í† í° 30ê°œ:\n{token_freq.most_common(30)}")


# ====== ì•„ë˜ ì½”ë“œë§Œ ì¶”ê°€ ======

import ast
import pandas as pd
df = raw.copy()

# tokens ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
def to_list_safe(x):
    if isinstance(x, list):
        return x
    try:
        return ast.literal_eval(x)
    except Exception:
        return []

df['tokens'] = df['tokens'].apply(to_list_safe)

# tokensë¥¼ í–‰ìœ¼ë¡œ ë¶„ë¦¬
long_df = df.explode('tokens', ignore_index=True)

# í•„ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ (place_name, visit_date, tokens)
long_df = long_df[['place_name', 'review_number','visit_date', 'tokens']].rename(columns={'tokens': 'token'})

# ë¹ˆ ê°’ ì œê±°
long_df = long_df[long_df['token'].notna() & (long_df['token'] != '')]

# CSV íŒŒì¼ë¡œ ì €ì¥
output_path = BASE_DIR / f"_tokens_long{TS}.csv"
long_df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"ì €ì¥ ì™„ë£Œ âœ… â†’ {output_path}")

