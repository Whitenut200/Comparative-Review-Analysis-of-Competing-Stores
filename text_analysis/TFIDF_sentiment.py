# í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸: TF-IDF â†’ ê°€ê²Œë³„ íŠ¹ì§• â†’ ê°ì •ë¶„ì„
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from collections import Counter
import re

# scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from scipy.stats import chi2_contingency

# ì„¤ì •
BASE_DIR = Path(r"D:/review+date+count")  # ë„¤ ê²½ë¡œ
TS = datetime.now().strftime("%Y%m%d_%H%M%S")

# KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ì¶”ê°€
sentiment_dict = {}
sentiment_dict_path = BASE_DIR / 'SentiWord_Dict.txt'

if sentiment_dict_path.exists():
    print(f"ê°ì„±ì‚¬ì „ ë¡œë“œ ì¤‘: {sentiment_dict_path.name}")
    with open(sentiment_dict_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) == 3:
                word = parts[0]
                pos_score = float(parts[1])
                neg_score = float(parts[2])
                sentiment_dict[word] = (pos_score, neg_score)
    print(f"ê°ì„±ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {len(sentiment_dict):,}ê°œ ë‹¨ì–´")
    USE_SENTIMENT_DICT = True
else:
    print("ê°ì„±ì‚¬ì „ íŒŒì¼ì´ ì—†ì–´ì„œ í‚¤ì›Œë“œ ë°©ì‹ë§Œ ì‚¬ìš©")
    USE_SENTIMENT_DICT = False

# í† í°í™”ëœ ë°ì´í„° ë¡œë“œ (token.pyì—ì„œ ìƒì„±ëœ íŒŒì¼)
# tfidfìš© ë°ì´í„° (ë¶ˆìš©ì–´ì œê±°)
token_tfidf_files = sorted(BASE_DIR.glob("reviews_tokens_tfidf_*.csv"))
if not token_tfidf_files:
    raise FileNotFoundError("token_tfidf_files í† í° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

# sentimentìš© ë°ì´í„° (ë¶ˆìš©ì–´ì œê±°X)
token_sentiment_files = sorted(BASE_DIR.glob("reviews_tokens_sentiment_*.csv"))
if not token_tfidf_files:
    raise FileNotFoundError("token_sentiment_files í† í° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

latest_token_file_tf = token_tfidf_files[-1]
tfidf_df = pd.read_csv(latest_token_file_tf)

latest_token_file_st = token_sentiment_files[-1]
sentiment_df = pd.read_csv(latest_token_file_st)

# NaN â†’ ë¹ˆë¬¸ì
sentiment_df["tokens_join"] = sentiment_df["tokens_join"].fillna("")

# TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
print("TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

# ì „ì²´ TF-IDF
vectorizer = TfidfVectorizer(
    max_features=1000, # ìƒìœ„ 1000ê°œ ë‹¨ì–´ê¹Œì§€ë§Œ ì‚¬ìš©
    min_df=2, # 2ê°œ ì´ìƒ ë¬¸ì„œì—ì„œ ë“±ì¥í•œ ë‹¨ì–´ë§Œ í¬í•¨
    max_df=0.9, # ì „ì²´ ë¬¸ì„œì˜ 90% ì´ìƒì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œê±° (ë„ˆë¬´ í”í•¨)
    ngram_range=(1, 2) # ë‹¨ì–´ 1ê°œ(uni-gram) + ë‹¨ì–´ìŒ(bi-gram) ëª¨ë‘ ê³ ë ¤
)
# NaN(Null) ì œê±° + ì´ëª¨í‹°ì½˜ë„ ì œì™¸
tf_df = tfidf_df.dropna(subset=['tokens_join']).copy()

# ë¹ˆ ë¬¸ìì—´(ê³µë°±ë§Œ í¬í•¨) ì œê±°
tf_df = tf_df[tf_df['tokens_join'].str.strip().ne('')]

# ìœ„ì— ì„¤ì •í•œëŒ€ë¡œ ë²¡í„°í™”
tfidf_matrix = vectorizer.fit_transform(tf_df['tokens_join']) 
# í–‰ë ¬ì˜ ì—´ ì´ë¦„(=ë‹¨ì–´ëª©ë¡) ê°€ì ¸ì˜¤ê¸°
feature_names = vectorizer.get_feature_names_out()

# ì „ì²´ í‚¤ì›Œë“œ ìˆœìœ„
tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()

# ë‹¨ì–´ì™€ ì ìˆ˜ í•œìŒìœ¼ë¡œ ë¬¶ì–´ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
global_keywords = [(feature_names[i], score) for i, score in enumerate(tfidf_scores)]
# ì ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ ìˆœì„œëŒ€ë¡œ ì •ë ¬
global_keywords.sort(key=lambda x: x[1], reverse=True)

# ê²°ê³¼ ì €ì¥ìš© DataFrame
# ìƒìœ„ 100ê°œë§Œ ì¶”ì¶œ
global_tfidf_df = pd.DataFrame(global_keywords[:100], columns=['keyword', 'tfidf_score'])
global_tfidf_df['rank'] = range(1, len(global_tfidf_df) + 1)
global_tfidf_df['analysis_type'] = 'global_tfidf'

print(f"ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ: {global_keywords[:10]}")

# ê°€ê²Œë³„ íŠ¹ì§• ë‹¨ì–´ ë¶„ì„
print("ê°€ê²Œë³„ íŠ¹ì§• ë‹¨ì–´ ë¶„ì„ ì¤‘...")

place_keywords = []
places = tf_df['place_name'].unique()

# ë¦¬ë·°í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
# ê°€ê²Œì˜ íŠ¹ì§•ì„ ì•Œì•„ë³´ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— (í•´ë‹¹ê°€ê²Œ vs ë‹¤ë¥¸ê°€ê²Œ) ë‘˜ë‹¤ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ
for place in places:
    place_reviews = tf_df[tf_df['place_name'] == place]['tokens_join']
    other_reviews = tf_df[tf_df['place_name'] != place]['tokens_join']
    
    if len(place_reviews) < 3:  # ë¦¬ë·°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
        continue
    
    # í•´ë‹¹ ê°€ê²Œ TF-IDF
    place_tfidf = vectorizer.transform(place_reviews)
    place_scores = np.array(place_tfidf.sum(axis=0)).flatten()
    
    # ë‹¤ë¥¸ ê°€ê²Œë“¤ TF-IDF  
    other_tfidf = vectorizer.transform(other_reviews)
    other_scores = np.array(other_tfidf.sum(axis=0)).flatten()
    
    # íŠ¹ì§•ë„ ê³„ì‚° (í•´ë‹¹ê°€ê²Œì ìˆ˜ / ì „ì²´í‰ê· ì ìˆ˜)
    total_scores = place_scores + other_scores
    distinctiveness = np.where(total_scores > 0, place_scores / (total_scores / len(places)), 0)
    
    # ìƒìœ„ í‚¤ì›Œë“œ ì„ ë³„
    top_indices = distinctiveness.argsort()[-20:][::-1]
    
    for idx in top_indices:
        if place_scores[idx] > 0:  # ì‹¤ì œ í•´ë‹¹ ê°€ê²Œì—ì„œ ì‚¬ìš©ëœ ë‹¨ì–´ë§Œ ê°€ì ¸ì˜¤ê¸°
            place_keywords.append({
                'place_name': place,
                'keyword': feature_names[idx],
                'place_tfidf': place_scores[idx],
                'distinctiveness': distinctiveness[idx],
                'analysis_type': 'place_distinctive'
            })

# ë°ì´í„°í”„ë¼ì„ í˜•íƒœë¡œ ì €ì¥
place_tfidf_df = pd.DataFrame(place_keywords)
place_tfidf_df = place_tfidf_df.sort_values(['place_name', 'distinctiveness'], ascending=[True, False])

print(f"ê°€ê²Œë³„ íŠ¹ì§• í‚¤ì›Œë“œ ìƒì„±: {len(place_tfidf_df)} ê±´")

# ê°ì • ë¶„ì„
print("ê°ì • ë¶„ì„ ì¤‘...")

# ê°ì„± ë‹¨ì–´ ì‚¬ì „
# ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì—¬ëŸ¬ë²ˆ ë³´ê°•í•¨
positive_words = set([
    # ê¸°ë³¸ ê¸ì •ì–´
    'ì¢‹', 'ë§›ìˆ', 'ìµœê³ ', 'í›Œë¥­', 'ì™„ë²½', 'ì¶”ì²œ', 'ë§Œì¡±', 'ê¹¨ë—', 'ì¹œì ˆ', 'ì‹ ì„ ',
    'ë§›ë‚˜', 'ë§›ì¢‹', 'ë§›ì§‘', 'ëë‚´ì£¼', 'êµ¿', 'ì¢‹ì•„', 'ì‚¬ë‘', 'í–‰ë³µ', 'ì¦ê±°', 'ê°€ì„±ë¹„',
    'ê³ ì†Œ', 'ë‹´ë°±', 'ê¹”ë”', 'ë¶€ë“œëŸ½', 'ì«„ê¹ƒ', 'ë°”ì‚­', 'ë‹¬ì½¤', 'í–¥ê¸‹', 'ë¹¨ë¦¬', 'ëƒ„ìƒˆì•ˆ',
    'í™˜ìƒ', 'ì˜ˆìˆ ', 'ì¼í’ˆ', 'ê°ë™', 'ë†€ë', 'ëŒ€ë°•', 'ì¸ìƒ',
    'í’ë¶€', 'ì§„í•˜', 'ì•Œë§', 'ë“ ë“ ', 'í¬ë§Œ', 'ì •ì„±', 'ì„¼ìŠ¤',
    'í‘¸ì§', 'ë„‰ë„‰', 'ì–‘ë§', 'ì‹ ì†', 'ë¹ ë¥´', 'ì •ê°ˆ', 'ìœ„ìƒ', 'ì„¸ì‹¬',
    'ê³ ê¸‰', 'í”„ë¦¬ë¯¸ì—„', 'íŠ¹ë³„', 'ë…íŠ¹', 'ìœ ë‹ˆí¬', 'ì°¨ë³„', 'ìƒˆë¡­',
    'ì°©í•œê°€ê²©', 'í˜œì', 

    # ì¶”ê°€ ê¸ì •ì–´
    'ìµœì• ', 'ì¬ë°©ë¬¸','ì ë‹¹','í•©ë¦¬', 'ì €ë ´','ê°€ì„±ë¹„','ê´œì°®', 'ë‚˜ì˜ì§€ì•Š',
  
    # ì¶”ê°€
    'ì¾Œì ', 'ì•ˆì§ˆê¸°', 'ì§œì§€ë„ì•ˆ', 'ì í•©','ê°•ì¶”','ìµœê³±'
])

negative_words = set([
    # ê¸°ë³¸ ë¶€ì •ì–´
    'ë§›ì—†', 'ë³„ë¡œ', 'ì‹¤ë§', 'ì§œ', 'ì‹±ê±°', 'ëŠ¦', 'ë¶ˆì¹œì ˆ', 'ë”ëŸ½',
    'ë¹„ì‹¸', 'ì•„ì‰½', 'í›„íšŒ', 'ìµœì•…', 'í™”ë‚˜', 'ë°ë°', 'í½í½', 'ì§ˆê¸°',
    'ëŠë¦¬', 'ì‹œë„ëŸ½', 'ë¶ˆí¸', 'ì°¨ê°€', 'ì‹', 'íƒ”', 'í˜¼ì¡', 'ëƒ„ìƒˆë‚˜','ë£¨ì¦ˆ', 'ë¶ˆê²°', 
    'ê³¼í•˜', 'ì‹¬í•˜', 'ë‚¡', 'í—ˆë¦„',
    # ì¶”ê°€ ë¶€ì •ì–´
    'ê·¸ëƒ¥ê·¸ë˜', 'í‰ë²”', 'ë¬´ë‚œ', 'ê·¸ì €ê·¸', 'ì˜ì˜', 'ì• ë§¤',
    'ì‘', 'ì ', 'ë¶€ì¡±', 'ëª¨ìë¼', 'ì•„ê¹',
    'ì§€ì €ë¶„', 'ìœ„ìƒì•ˆ', 'ì˜¤ë˜ë',
    'ë¬´ì„±ì˜', 'ë¶ˆë§Œ', 'ì§œì¦', 'í™©ë‹¹', 'ì–´ì´ì—†', 'ê¸°ëŒ€ì´í•˜',
    'í—ˆìˆ ', 'ì—‰ë§', 'ê°œíŒ', 'ì¡°ì¡', 'ê¸€ì„'
])

# ê°ì„± ë¶„ì„ì—ì„œ ì œì™¸í•  ì¤‘ë¦½ ë‹¨ì–´ë“¤
neutral_exclude_words = set([
    # ìŒì‹ ê´€ë ¨ ì¤‘ë¦½ì–´
    'ì‹ì‚¬', 'ì‹êµ¬', 'íšŒì‹', 'í›„ì‹', 'ì™¸ì‹', 'ìŒì‹', 'í­ì‹', 
    'ì§œì¥', 'ë©”ë‰´', 'ê°€ê²Œ', 'ì‹ë‹¹', 'ì‹', 'ìŒ',
    'ìš”ë¦¬', 'ë°˜ì°¬', 'ë°¥', 'êµ­', 'ì°Œê°œ', 'ì „', 'êµ¬ì´', 'ë³¶ìŒ',
    
    # ì‹œì„¤ ê´€ë ¨ ì¤‘ë¦½ì–´
    'ì£¼ì°¨ê°€ëŠ¥', 'ì£¼ì°¨', 'í¬ì¥', 'ë°°ë‹¬', 'ì˜ˆì•½', 'ëŒ€ê¸°',
    
    # í¬ê¸°/ì–‘ ì¤‘ë¦½ì–´
    'ì‘', 'ì ', 'í¬', 'ë§', 'ì–‘',
    
    # ê¸°íƒ€
    'ìƒê°', 'ëŠë‚Œ', 'ê²½ìš°', 'ì •ë„', 'í¸', 'ì‹œê°„', 'ì¥ì†Œ','ê³µì§œ','ì§„ì§œ'])


# ë¶€ì •ì–´ ë¦¬ìŠ¤íŠ¸
negation_words = {'ì•ˆ', 'ëª»', 'ì—†', 'ì•„ë‹ˆ', 'ì „í˜€', 'ê²°ì½”', 'ë¹„'}

# ê°•ì¡°ì–´ ê°€ì¤‘ì¹˜
intensifiers = {
    'ë„ˆë¬´': 1.5, 'ì •ë§': 1.5, 'ì§„ì§œ': 1.5, 'ì™„ì „': 1.5,
    'ë§¤ìš°': 1.3, 'ì•„ì£¼': 1.3, 'ì—„ì²­': 1.5, 'ì§„ì‹¬': 1.5,
    'ìµœê³ ë¡œ': 2.0, 'ê·¹ë„ë¡œ': 2.0, 'ê·¹': 1.8, 'ë˜ê²Œ': 1.3,
    'ì—„ì²­ë‚˜': 1.5, 'ì™„ì „íˆ': 1.5, 'ì •ë§ë¡œ': 1.5
}

def analyze_sentiment(tokens_str):
    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    try:
        import pandas as pd
    except Exception:
        pd = None

    if tokens_str is None:
        tokens_str = ""
    
    elif not isinstance(tokens_str, str):
        # NaN(float) ì²˜ë¦¬
        if pd is not None and isinstance(tokens_str, float) and pd.isna(tokens_str):
            tokens_str = ""
        # ë¦¬ìŠ¤íŠ¸ë©´ ê³µë°± ì¡°ì¸
        elif isinstance(tokens_str, list):
            tokens_str = " ".join(tokens_str)
        else:
            tokens_str = str(tokens_str)

    # ìª¼ê°œê¸°
    tokens = tokens_str.split()
    
    pos_score = 0
    neg_score = 0
    matched_pos = []
    matched_neg = []
    
    for i, token in enumerate(tokens):
        # ì¤‘ë¦½ ë‹¨ì–´ ìŠ¤í‚µ
        if any(exclude in token for exclude in neutral_exclude_words):
            continue
        
        # ê°•ë„ ë° ë¶€ì •ì–´ í™•ì¸
        intensity = 1.0 # ê¸°ë³¸ ê°ì • ì ìˆ˜ : 1.0
        has_negation = False # ê¸°ë³¸ì€ ë¶€ì •ì–´ê°€ ì•„ë‹˜
        if i > 0:
            prev_token = tokens[i-1] # í˜„ì¬ ë‹¨ì–´ ë°”ë¡œ ì• ë‹¨ì–´ë¥¼ ê°€ì ¸ì˜´
            if prev_token in intensifiers: # ê°•ì¡°ì–´ê°€ ìˆìœ¼ë©´
                intensity = intensifiers[prev_token] # í•´ë‹¹ ê°•ë„ë¡œ ê°’ì„ ë°”ê¿ˆ
            if prev_token in negation_words: # ë¶€ì •ì–´ê°€ ìˆìœ¼ë©´
                has_negation = True # ë¶€ì •ìœ¼ë¡œ êµ¬ë¶„ë¶„
        
        # 1ìˆœìœ„: ê°ì„±ì‚¬ì „ ì‚¬ìš©
        if USE_SENTIMENT_DICT and token in sentiment_dict:
            dict_pos, dict_neg = sentiment_dict[token]
            
            # ë¶€ì •ì–´ ì²˜ë¦¬ (ì˜ë¯¸ ë°˜ì „)
            if has_negation:
                dict_pos, dict_neg = dict_neg, dict_pos
            
            # ì ìˆ˜ ì ìš©
            if dict_pos > dict_neg:
                pos_score += dict_pos * intensity
                matched_pos.append(token)
            elif dict_neg > dict_pos:
                neg_score += dict_neg * intensity
                matched_neg.append(token)
        
        # 2ìˆœìœ„: í‚¤ì›Œë“œ ë°©ì‹ (ê°ì„±ì‚¬ì „ì— ì—†ì„ ë•Œ)
        else:
            is_positive = any(pw in token for pw in positive_words)
            is_negative = any(nw in token for nw in negative_words)

            # ê¸ì •ì–´ ë¦¬ìŠ¤íŠ¸ì— ìˆì„ ë•Œ
            if is_positive:
                if has_negation:
                    neg_score += intensity
                    matched_neg.append(f"{tokens[i-1]} {token}")
                else:
                    pos_score += intensity
                    matched_pos.append(token)

            # ë¶€ì •ì–´ ë¦¬ìŠ¤íŠ¸ì— ìˆì„ ë•Œë•Œ
            elif is_negative:
                if has_negation:
                    pos_score += intensity
                    matched_pos.append(f"{tokens[i-1]} {token}")
                else:
                    neg_score += intensity
                    matched_neg.append(token)

    # ê°ì •ì ìˆ˜ ì‹
    sentiment_score = pos_score - neg_score

    # ê¸ì •ì–´/ë¶€ì •ì–´ êµ¬ë¶„ë¶„
    if sentiment_score > 0.5:
        return 'positive', pos_score, neg_score, matched_pos, matched_neg
    elif sentiment_score < -0.5:
        return 'negative', pos_score, neg_score, matched_pos, matched_neg
    else:
        return 'neutral', pos_score, neg_score, matched_pos, matched_neg

# ê°ì • ë¶„ì„ ì ìš©
sentiment_results = []
for idx, row in sentiment_df.iterrows():
    sentiment, pos_score, neg_score, matched_pos, matched_neg = analyze_sentiment(row['tokens_join'])
    sentiment_results.append({
        'place_name': row['place_name'],
        'sentiment': sentiment,
        'positive_score': round(pos_score, 2),  # scoreë¡œ ë³€ê²½
        'negative_score': round(neg_score, 2),  # scoreë¡œ ë³€ê²½
        'sentiment_score': round(pos_score - neg_score, 2),
        'matched_positive_words': ', '.join(matched_pos),
        'matched_negative_words': ', '.join(matched_neg)
    })

sent_df = pd.DataFrame(sentiment_results)

# ê°€ê²Œë³„ ê°ì • ìš”ì•½
place_sentiment = sent_df.groupby('place_name').agg({
    'sentiment_score': ['mean', 'std', 'count'],
    'positive_score': 'sum',
    'negative_score': 'sum',
    'matched_positive_words': lambda x: ', '.join([w for w in x if w]),
    'matched_negative_words': lambda x: ', '.join([w for w in x if w])
}).round(3)

place_sentiment.columns = [
    'avg_sentiment_score', 'sentiment_std', 'review_count',
    'total_positive_score', 'total_negative_score',
    'all_positive_keywords', 'all_negative_keywords'
]

place_sentiment = place_sentiment.reset_index()

print(f"ê°ì • ë¶„ì„ ì™„ë£Œ: {len(sent_df)} ë¦¬ë·°")
print(f"ê¸ì •: {len(sent_df[sent_df['sentiment']=='positive'])} / "
      f"ë¶€ì •: {len(sent_df[sent_df['sentiment']=='negative'])} / "
      f"ì¤‘ë¦½: {len(sent_df[sent_df['sentiment']=='neutral'])}")


# ê²°ê³¼ ì €ì¥
print("íƒœë¸”ë¡œìš© ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")

# íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
# ë°ì´í„° ë®ì–´ì”Œì—¬ì§€ê¸° ë°©ì§€
outputs = {
    f'tableau_global_keywords_{TS}.csv': global_tfidf_df,
    f'tableau_place_keywords_{TS}.csv': place_tfidf_df,
    f'tableau_sentiment_by_review_{TS}.csv': sent_df,
    f'tableau_sentiment_by_place_{TS}.csv': place_sentiment,
    # f'tableau_topic_words_{TS}.csv': topic_df,
    # f'tableau_review_topics_{TS}.csv': doc_topic_df,
    # f'tableau_place_topic_summary_{TS}.csv': place_topic_summary
}

saved_files = []
for filename, dataframe in outputs.items():
    filepath = BASE_DIR / filename
    dataframe.to_csv(filepath, index=False, encoding='utf-8-sig')
    saved_files.append(filename)
    print(f"  âœ… {filename}: {len(dataframe)} í–‰")

print("ë¶„ì„ ì™„ë£Œ! íƒœë¸”ë¡œìš© íŒŒì¼:")
for f in saved_files:
    print(f"  ğŸ“ {f}")

# ì¶”ê°€ ë°ì´í„° ìƒì„±(ë„êµ¬ì„¤ëª…ìš©)
import pandas as pd

# ë°ì´í„° ê³ ëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°
tmp = sent_df.copy()

# ë¬¸ìì—´ -> ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ì•ˆì „ ë³€í™˜
def split_words(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return []
    if isinstance(x, (list, set, tuple)):
        parts = list(x)
    else:
        parts = str(x).split(',')  # ì½¤ë§ˆ ê¸°ì¤€ ë¶„ë¦¬
    # ê³µë°±/ë¹ˆê°’ ì œê±°
    parts = [p.strip() for p in parts if p and p.strip()]
    return parts

# place_name, word_info(positive/negative), words í˜•íƒœë¡œ ë³€í™˜
tmp = tmp.reset_index(drop=True)
tmp["review_number"] = tmp.index + 1
tmp = tmp[['place_name', 'review_number', 'matched_positive_words', 'matched_negative_words']]

rows = []
for _, r in tmp.iterrows():
    # ê¸ì • ë‹¨ì–´
    for w in split_words(r['matched_positive_words']):
        rows.append({
            'place_name': r['place_name'],
            'review_number': r['review_number'],
            'word_info': 'positive',
            'words': w
        })
    # ë¶€ì • ë‹¨ì–´
    for w in split_words(r['matched_negative_words']):
        rows.append({
            'place_name': r['place_name'],
            'review_number': r['review_number'],
            'word_info': 'negative',
            'words': w
        })

place_word_long = pd.DataFrame(rows)

# ì¤‘ë³µ ì œê±°(ì›í•˜ë©´ ìœ ì§€í•´ë„ ë¨)
place_word_long = place_word_long.drop_duplicates()

# ì €ì¥
filename= 'place_word_long.csv'
filepath = BASE_DIR / filename
place_word_long.to_csv(filepath, index=False, encoding='utf-8-sig')
print(f'ì €ì¥ ì™„ë£Œ â†’ {filepath}')
